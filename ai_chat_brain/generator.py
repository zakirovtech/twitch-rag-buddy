from __future__ import annotations

import asyncio
import logging

import requests

from models import GenerationRequest, Settings, ChatItem


def _format_recent(recent: list[ChatItem] | None, max_n: int = 15) -> str:
    if not recent:
        return ""
    msgs = recent[-max_n:]
    return "\n".join(f"{m.user}: {m.text}" for m in msgs)


class BaseGenerator:
    async def generate(self, req: GenerationRequest) -> str:
        raise NotImplementedError


class RuleBasedGenerator(BaseGenerator):
    """Non-LLM fallback. Keeps things short."""

    async def generate(self, req: GenerationRequest) -> str:
        s = req.summary
        topic = s.topic if s else "Ñ‡Ð°Ñ‚"

        if req.purpose == "answer_ai" and req.user_text:
            return (
                f"ÐŸÐ¾Ð½ÑÐ» Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ñ€Ð¾ {topic}. Ð¯ Ð¿Ð¾ÐºÐ° Ð±ÐµÐ· RAG, Ð½Ð¾ ÑƒÑ‚Ð¾Ñ‡Ð½ÑŽ: "
                f"Ñ‚ÐµÐ±Ðµ Ð½ÑƒÐ¶ÐµÐ½ Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´ Ð¸Ð»Ð¸ Ñ€Ð°Ð·Ð±Ð¾Ñ€ Ð¿Ð¾ ÑˆÐ°Ð³Ð°Ð¼?"
            )

        if req.purpose == "mention":
            if req.user:
                return f"@{req.user} Ñ Ñ‚ÑƒÑ‚ ðŸ‘€ ÐŸÑ€Ð¾ {topic} â€” Ñ‡Ñ‚Ð¾ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð¾Ð±ÑÑƒÐ´Ð¸Ñ‚ÑŒ?"
            return f"Ð¯ Ñ‚ÑƒÑ‚ ðŸ‘€ ÐŸÑ€Ð¾ {topic} â€” Ñ‡Ñ‚Ð¾ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð¾Ð±ÑÑƒÐ´Ð¸Ñ‚ÑŒ?"

        # initiate
        if s and s.questions:
            q = s.questions[0]
            return f"ÐšÑÑ‚Ð°Ñ‚Ð¸, Ð¿Ð¾ Ñ‚ÐµÐ¼Ðµ ({topic}): {q[:120]}{'â€¦' if len(q) > 120 else ''}"

        return f"Ð¡Ð»ÑƒÑˆÐ°ÑŽ Ñ‡Ð°Ñ‚ Ð¿Ñ€Ð¾ {topic}. Ð•ÑÐ»Ð¸ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ â€” Ð·Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ñ‡ÐµÑ€ÐµÐ· !ai â€¦"


class OllamaGenerator(BaseGenerator):
    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self.log = logging.getLogger("ollama")

    def _build_messages(self, req: GenerationRequest) -> list[dict]:
        s = req.summary
        recent_txt = _format_recent(req.recent, max_n=self.settings.max_context_msgs)

        system = (
            "Ð¢Ñ‹ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸Ðº Ñ‡Ð°Ñ‚Ð° Twitch-ÑÑ‚Ñ€Ð¸Ð¼Ð°. "
            "ÐŸÐ¸ÑˆÐ¸ ÐžÐ”ÐÐž ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ (1â€“2 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ), Ð±ÐµÐ· Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð½ÐµÐ¹, Ð±ÐµÐ· ÑÑÑ‹Ð»Ð¾Ðº, Ð±ÐµÐ· Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸. "
            "ÐÐµ ÑÐ¿Ð°Ð¼ÑŒ ÑÐ¼Ð¾Ð´Ð·Ð¸. ÐÐµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ¹ÑÑ. "
            "Ð•ÑÐ»Ð¸ Ð½Ðµ Ñ…Ð²Ð°Ñ‚Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° â€” Ð·Ð°Ð´Ð°Ð¹ Ð¾Ð´Ð¸Ð½ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÑŽÑ‰Ð¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ."
        )

        if req.purpose == "initiate":
            user = (
                f"Ð¢ÐµÐºÑƒÑ‰Ð°Ñ Ñ‚ÐµÐ¼Ð° Ñ‡Ð°Ñ‚Ð°: {s.topic if s else 'Ñ‡Ð°Ñ‚'}\n"
                f"ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°: {', '.join(s.keywords[:8]) if s and s.keywords else ''}\n"
                f"Ð’Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð² Ñ‡Ð°Ñ‚Ðµ: {(' | '.join(s.questions[:3])) if s and s.questions else ''}\n\n"
                f"ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ:\n{recent_txt}\n\n"
                "Ð¡Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÐ¹ ÑƒÐ¼ÐµÑÑ‚Ð½ÑƒÑŽ Ñ€ÐµÐ¿Ð»Ð¸ÐºÑƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€ Ð¿Ð¾ Ñ‚ÐµÐ¼Ðµ."
            )
        elif req.purpose == "mention":
            user = (
                f"Ð¢ÐµÐ±Ñ ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÐ»Ð¸ Ð² Ñ‡Ð°Ñ‚Ðµ. ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ: {req.user or ''}\n"
                f"Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: {req.user_text or ''}\n\n"
                f"ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚/Ñ‚ÐµÐ¼Ð°: {s.topic if s else 'Ñ‡Ð°Ñ‚'}\n"
                f"ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ:\n{recent_txt}\n\n"
                "ÐžÑ‚Ð²ÐµÑ‚ÑŒ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ (1 ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ)."
            )
        else:  # answer_ai
            user = (
                f"ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð·Ð°Ð´Ð°Ñ‘Ñ‚ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ñ‡ÐµÑ€ÐµÐ· !ai. ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ: {req.user or ''}\n"
                f"Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {req.user_text or ''}\n\n"
                f"Ð¢ÐµÐ¼Ð° Ñ‡Ð°Ñ‚Ð°: {s.topic if s else 'Ñ‡Ð°Ñ‚'}\n"
                f"ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ:\n{recent_txt}\n\n"
                "Ð”Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ (1â€“2 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ)."
            )

        return [{"role": "system", "content": system}, {"role": "user", "content": user}]

    def _call_ollama_sync(self, req: GenerationRequest) -> str:
        url = self.settings.ollama_url.rstrip("/")
        payload = {
            "model": self.settings.ollama_model,
            "messages": self._build_messages(req),
            "stream": False,
            "options": {"temperature": self.settings.ollama_temperature},
        }

        resp = requests.post(f"{url}/api/chat", json=payload, timeout=45)
        resp.raise_for_status()
        data = resp.json()
        content = ((data.get("message") or {}).get("content") or data.get("response") or "").strip()
        return content

    async def generate(self, req: GenerationRequest) -> str:
        try:
            text = await asyncio.to_thread(self._call_ollama_sync, req)
        except Exception as e:
            self.log.warning("Ollama failed (%s). Falling back.", e)
            return await RuleBasedGenerator().generate(req)

        text = " ".join(text.split())
        if len(text) > req.max_len:
            text = text[: req.max_len].rsplit(" ", 1)[0] + "â€¦"
        return text


def build_generator(settings: Settings) -> BaseGenerator:
    if settings.ollama_url:
        return OllamaGenerator(settings)
    return RuleBasedGenerator()
