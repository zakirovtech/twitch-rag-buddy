from __future__ import annotations

import asyncio
import logging
import re
from copy import deepcopy

import requests

from models import GenerationRequest, Settings, ChatItem


def _format_recent(recent: list[ChatItem] | None, max_n: int = 15) -> str:
    if not recent:
        return ""
    msgs = recent[-max_n:]
    return "\n".join(f"{m.user}: {m.text}" for m in msgs)


# –õ—ë–≥–∫–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ª–æ–≤–∏–º ‚Äú–¥—Ä–µ–π—Ñ‚Äù –≤ CJK (–∫–∏—Ç–∞–π—Å–∫–∏–π/—è–ø–æ–Ω—Å–∫–∏–π/–∫–æ—Ä–µ–π—Å–∫–∏–π).
_CJK_RE = re.compile(r"[\u4e00-\u9fff\u3040-\u30ff\uac00-\ud7af]")
_CYR_RE = re.compile(r"[–ê-–Ø–∞-—è–Å—ë]")
_LAT_RE = re.compile(r"[A-Za-z]")


def _looks_russian(text: str) -> bool:
    if not text:
        return True
    if _CJK_RE.search(text):
        return False
    cyr = len(_CYR_RE.findall(text))
    lat = len(_LAT_RE.findall(text))
    if cyr == 0 and lat == 0:  # —ç–º–æ–¥–∑–∏/–∑–Ω–∞–∫–∏
        return True
    return cyr >= max(1, lat * 2)


class BaseGenerator:
    async def generate(self, req: GenerationRequest) -> str:
        raise NotImplementedError


class RuleBasedGenerator(BaseGenerator):
    """Non-LLM fallback. Keeps things short."""

    async def generate(self, req: GenerationRequest) -> str:
        s = req.summary
        topic = s.topic if s else "—á–∞—Ç"

        if req.purpose == "answer_ai" and req.user_text:
            return (
                f"–ü–æ–Ω—è–ª –≤–æ–ø—Ä–æ—Å –ø—Ä–æ {topic}. –Ø –ø–æ–∫–∞ –±–µ–∑ RAG, –Ω–æ —É—Ç–æ—á–Ω—é: "
                f"—Ç–µ–±–µ –Ω—É–∂–µ–Ω –±—ã—Å—Ç—Ä—ã–π –≤—ã–≤–æ–¥ –∏–ª–∏ —Ä–∞–∑–±–æ—Ä –ø–æ —à–∞–≥–∞–º?"
            )

        if req.purpose == "mention":
            if req.user:
                return f"@{req.user} —è —Ç—É—Ç üëÄ –ü—Ä–æ {topic} ‚Äî —á—Ç–æ –∏–º–µ–Ω–Ω–æ –æ–±—Å—É–¥–∏—Ç—å?"
            return f"–Ø —Ç—É—Ç üëÄ –ü—Ä–æ {topic} ‚Äî —á—Ç–æ –∏–º–µ–Ω–Ω–æ –æ–±—Å—É–¥–∏—Ç—å?"

        if s and s.questions:
            q = s.questions[0]
            return f"–ö—Å—Ç–∞—Ç–∏, –ø–æ —Ç–µ–º–µ ({topic}): {q[:120]}{'‚Ä¶' if len(q) > 120 else ''}"

        return f"–°–ª—É—à–∞—é —á–∞—Ç –ø—Ä–æ {topic}. –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ ‚Äî –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å —á–µ—Ä–µ–∑ !ai ‚Ä¶"


class OllamaGenerator(BaseGenerator):
    def __init__(self, settings: Settings) -> None:
        self.settings = settings
        self.log = logging.getLogger("ollama")

    def _build_messages(self, req: GenerationRequest) -> list[dict]:
        s = req.summary
        recent_txt = _format_recent(req.recent, max_n=self.settings.max_context_msgs)

        system = (
            "–¢—ã —É—á–∞—Å—Ç–Ω–∏–∫ —á–∞—Ç–∞ Twitch-—Å—Ç—Ä–∏–º–∞. "
            "–ü–∏—à–∏ –û–î–ù–û –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –±–µ–∑ –ø—Ä–æ—Å—Ç—ã–Ω–µ–π, –±–µ–∑ —Å—Å—ã–ª–æ–∫, –±–µ–∑ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏. "
            "–ù–µ —Å–ø–∞–º—å —ç–º–æ–¥–∑–∏. –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π—Å—è. "
            "–ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ‚Äî –∑–∞–¥–∞–π –æ–¥–∏–Ω —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å. "
            "–ù–ï –ü–ò–®–ò —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è/chain-of-thought. –í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç."
        )
        if self.settings.ollama_force_ru:
            system += (
                " –í–ê–ñ–ù–û: –æ—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
                "–ó–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∏—Ç–∞–π—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π. "
                "–ï—Å–ª–∏ –Ω–∞—á–∞–ª –Ω–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º ‚Äî –ø–µ—Ä–µ–ø–∏—à–∏ –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º."
            )

        if req.purpose == "initiate":
            user = (
                f"–¢–µ–∫—É—â–∞—è —Ç–µ–º–∞ —á–∞—Ç–∞: {s.topic if s else '—á–∞—Ç'}\n"
                f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(s.keywords[:8]) if s and s.keywords else ''}\n"
                f"–í–æ–ø—Ä–æ—Å—ã –≤ —á–∞—Ç–µ: {(' | '.join(s.questions[:3])) if s and s.questions else ''}\n\n"
                f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:\n{recent_txt}\n\n"
                "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —É–º–µ—Å—Ç–Ω—É—é —Ä–µ–ø–ª–∏–∫—É, —á—Ç–æ–±—ã –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä –ø–æ —Ç–µ–º–µ."
            )
        elif req.purpose == "mention":
            user = (
                f"–¢–µ–±—è —É–ø–æ–º—è–Ω—É–ª–∏ –≤ —á–∞—Ç–µ. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {req.user or ''}\n"
                f"–°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {req.user_text or ''}\n\n"
                f"–ö–æ–Ω—Ç–µ–∫—Å—Ç/—Ç–µ–º–∞: {s.topic if s else '—á–∞—Ç'}\n"
                f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:\n{recent_txt}\n\n"
                "–û—Ç–≤–µ—Ç—å –∫–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É (1 —Å–æ–æ–±—â–µ–Ω–∏–µ)."
            )
        else:  # answer_ai
            user = (
                f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å —á–µ—Ä–µ–∑ !ai. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {req.user or ''}\n"
                f"–í–æ–ø—Ä–æ—Å: {req.user_text or ''}\n\n"
                f"–¢–µ–º–∞ —á–∞—Ç–∞: {s.topic if s else '—á–∞—Ç'}\n"
                f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:\n{recent_txt}\n\n"
                "–î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)."
            )

        return [{"role": "system", "content": system}, {"role": "user", "content": user}]

    def _post_chat(self, payload: dict) -> dict:
        url = self.settings.ollama_url.rstrip("/")
        resp = requests.post(
            f"{url}/api/chat",
            json=payload,
            timeout=self.settings.ollama_timeout_sec,
        )
        resp.raise_for_status()
        return resp.json()

    @staticmethod
    def _extract(data: dict) -> tuple[str, str, str]:
        msg = data.get("message") or {}
        content = (msg.get("content") or data.get("response") or "").strip()
        thinking = (msg.get("thinking") or "").strip()
        done_reason = (data.get("done_reason") or "").strip()
        return content, thinking, done_reason

    def _call_ollama_sync(self, req: GenerationRequest) -> str:
        # –ï—Å–ª–∏ –≤ Settings –Ω–µ—Ç ollama_think ‚Äî –¥–µ—Ñ–æ–ª—Ç False (—á—Ç–æ–±—ã –Ω–µ —Å–∂–∏—Ä–∞–ª–æ num_predict –Ω–∞ thinking)
        think_flag = bool(getattr(self.settings, "ollama_think", False))

        base_payload = {
            "model": self.settings.ollama_model,
            "messages": self._build_messages(req),
            "stream": False,
            "think": think_flag,  # <-- –∫–ª—é—á–µ–≤–æ–π —Ñ–∏–∫—Å –¥–ª—è thinking-–º–æ–¥–µ–ª–µ–π
            "options": {
                "temperature": self.settings.ollama_temperature,
                "num_ctx": self.settings.ollama_num_ctx,
                "num_predict": self.settings.ollama_num_predict,
                "top_p": self.settings.ollama_top_p,
                "repeat_penalty": self.settings.ollama_repeat_penalty,
            },
        }

        data = self._post_chat(base_payload)
        if isinstance(data, dict) and data.get("error"):
            raise RuntimeError(f"Ollama error: {data['error']}")

        content, thinking, done_reason = self._extract(data)

        # –ï—Å–ª–∏ thinking –≤–∫–ª—é—á—ë–Ω (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —É —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π) –∏ num_predict –º–∞–ª ‚Äî content –º–æ–∂–µ—Ç –Ω–µ –ø–æ—è–≤–∏—Ç—å—Å—è.
        # –î–µ–ª–∞–µ–º 1 retry: think=false + num_predict –ø–æ–±–æ–ª—å—à–µ.
        if not content or done_reason == "length":
            retry = deepcopy(base_payload)
            retry["think"] = False  # –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–∫–ª—é—á–∞–µ–º thinking –Ω–∞ —Ä–µ—Ç—Ä–∞–µ
            retry["options"]["temperature"] = min(0.2, float(retry["options"]["temperature"]))
            retry["options"]["num_predict"] = max(int(self.settings.ollama_num_predict), 192)
            retry["messages"][0]["content"] += (
                " –°–ï–ô–ß–ê–° –í–ï–†–ù–ò –¢–û–õ–¨–ö–û –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢ (–ë–ï–ó –†–ê–°–°–£–ñ–î–ï–ù–ò–ô)."
            )

            data2 = self._post_chat(retry)
            if isinstance(data2, dict) and data2.get("error"):
                raise RuntimeError(f"Ollama error: {data2['error']}")

            content2, thinking2, done_reason2 = self._extract(data2)
            if content2:
                content = content2
                thinking = thinking2
                done_reason = done_reason2
            else:
                self.log.warning(
                    "Ollama empty content (raw). done_reason=%s/%s thinking=%r/%r raw1=%r raw2=%r",
                    done_reason,
                    done_reason2,
                    (thinking[:200] if thinking else ""),
                    (thinking2[:200] if thinking2 else ""),
                    data,
                    data2,
                )
                raise RuntimeError("Empty content from Ollama (thinking-only or truncated)")

        # RU retry (—Ç–≤–æ—è –ª–æ–≥–∏–∫–∞)
        if (
            self.settings.ollama_force_ru
            and self.settings.ollama_retry_non_ru
            and content
            and not _looks_russian(content)
        ):
            retry = deepcopy(base_payload)
            retry["think"] = False
            retry["options"]["temperature"] = min(0.2, float(retry["options"]["temperature"]))
            retry["options"]["num_predict"] = max(int(self.settings.ollama_num_predict), 192)
            retry["messages"][0]["content"] += (
                " –°–ï–ô–ß–ê–° –í–ï–†–ù–ò –†–û–í–ù–û –û–î–ù–û –°–û–û–ë–©–ï–ù–ò–ï –ù–ê –†–£–°–°–ö–û–ú. –ù–ò–ö–ê–ö–ò–• –î–†–£–ì–ò–• –Ø–ó–´–ö–û–í."
            )
            data3 = self._post_chat(retry)
            if isinstance(data3, dict) and data3.get("error"):
                raise RuntimeError(f"Ollama error: {data3['error']}")
            content3, _, _ = self._extract(data3)
            if content3:
                return content3

        return content

    async def generate(self, req: GenerationRequest) -> str:
        try:
            text = await asyncio.to_thread(self._call_ollama_sync, req)
        except Exception as e:
            self.log.warning("Ollama failed (%s). Falling back.", e)
            return await RuleBasedGenerator().generate(req)

        text = " ".join(text.split())
        if not text:
            return await RuleBasedGenerator().generate(req)

        if len(text) > req.max_len:
            text = text[: req.max_len].rsplit(" ", 1)[0] + "‚Ä¶"
        return text


def build_generator(settings: Settings) -> BaseGenerator:
    if settings.ollama_url:
        return OllamaGenerator(settings)
    return RuleBasedGenerator()
